model_args:
  model_name_or_path: roneneldan/TinyStories-1Layer-21M
  # cache_dir: ~/data/.cache/huggingface
data_args:
  dataset_name: roneneldan/TinyStories
  # dataset_config_name: wikitext-103-v1
  streaming: False
training_args:
  run_name: tinystories_1layer_attn_mlp_C10k_k8
  output_dir: output_main
  overwrite_output_dir: True
  do_train: True
  do_eval: True
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  learning_rate: 5e-4
  warmup_ratio: 0.05
  lr_scheduler_type: linear
  max_steps: 100000
  logging_first_step: True
  logging_steps: 50
  evaluation_strategy: steps
  eval_steps: 500
  save_steps: 500
  save_total_limit: 2
  load_best_model_at_end: True
  ddp_find_unused_parameters: False
  tf32: False
  dataloader_num_workers: 4

  train_model_params: True
  model_lr_factor: 1
  codebook_reg_p: null
  codebook_weight_decay: 0.01

codebook_args:
  codebook_at: ["attn", "mlp"]
  codebook_type: ["vanilla", "vanilla"]
  num_codebooks: 1
  # codebook_at: "attn_preproj"
  # codebook_type: "group"
  # num_codebooks: -1
  num_codes: 10000
  layers_to_snap: all
  similarity_metric: inner_product
  loss: aeloss
  k_codebook: 100
  kmeans_init: False
  kmeans_path: /.cache/cb_volume/huggingface/kmeans_embeddings.pt
  kmeans_init_examples: 1000
  kmeans_kwargs:
    n_init: auto
    batch_size: 24576
  codebook_kwargs: null
  replace_codes: False

k_scheduler_kwargs: null

enable_logging: True
pretrained_path: null
get_baseline: False
tag_keys: []
tags: []
project: tinystories_1layer
